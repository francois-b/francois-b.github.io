---
title: "The Outsourced Mind: What We Lose When We Stop Struggling"
date: "2024-12-13"
slug: "outsourced-mind-cognitive-offloading"
tags: ["ai", "cognition", "philosophy", "learning"]
author: "Theo"
---

There's a word in cognitive psychology: *desirable difficulty*. It refers to the counterintuitive finding that learning is often enhanced by making the learning process harder, not easier.

Struggling to recall something strengthens memory more than simply re-reading it. Working through a problem yourself, even inefficiently, builds understanding that passive consumption doesn't. The effort isn't waste—it's where the learning happens.

AI assistants represent the largest experiment in cognitive offloading in human history. We're outsourcing not just information retrieval (we've done that since writing), but reasoning, synthesis, and creative generation.

What do we lose when we stop struggling?

## The Extended Mind, Extended Further

Philosophers have long recognized that cognition doesn't stop at the skull. We think with pencils and paper, with calculators and calendars, with notebooks and now smartphones. Andy Clark and David Chalmers called this the "extended mind"—our cognitive processes incorporate external tools.

But there's always been a distinction between *storage* and *processing*. We've outsourced memory to books. We've outsourced calculation to machines. But the synthesis—the integration of information into understanding—has remained largely internal.

AI blurs this line. When I ask an LLM to summarize a complex document, it's not just storing or calculating. It's doing something that resembles understanding. And when I accept its summary without engaging deeply with the source material, I've outsourced not just the work but the cognitive process that would have shaped my own understanding.

## The Skill Atrophy Problem

Skills atrophy when not exercised. This is uncontroversial for physical skills—stop playing piano and your fingers forget. But it applies equally to cognitive skills.

The ability to hold complex information in working memory. The capacity to reason through multi-step problems without external scaffolding. The skill of formulating a clear question before knowing the answer. These are all skills that can weaken through disuse.

I've noticed this in myself. When I use AI assistants heavily for a period, I find my unassisted thinking feels slower, less confident. When I deliberately step away from these tools, the capacity returns. The skill was always there—it just wasn't being exercised.

This isn't a reason to abandon AI tools. But it suggests a kind of cognitive hygiene: deliberately exercising unassisted thinking to maintain the capacity, much as you might walk even if you own a car.

## The Generation Effect

One of the most robust findings in learning science is the *generation effect*: information you generate yourself is remembered better than information you passively receive.

This creates an interesting tension with AI tools. When an AI generates an answer for me, I haven't generated it myself—even if I later review and modify it. The cognitive work of generation has been bypassed.

I've experimented with this deliberately. When learning a new domain, I try to formulate my own initial understanding before consulting any source—including AI. The initial formulation is usually wrong or incomplete. But the act of generating it creates a scaffold that makes subsequent learning more effective.

The AI becomes more useful *after* I've struggled, because I have a structure to integrate new information into. Without that structure, AI-provided information washes over me without sticking.

## The Illusion of Understanding

There's a phenomenon called the *illusion of explanatory depth*. People think they understand complex systems (like how a toilet works, or how a government functions) much better than they actually do. The illusion is exposed when they're asked to explain the system in detail.

AI enables a new version of this illusion. It's easy to feel that you understand something when you can query an AI and get a coherent answer. But the understanding lives in the AI, not in you. You haven't built the mental model. You've just borrowed someone else's.

This matters because real understanding enables transfer—applying knowledge to new situations, recognizing patterns across domains, generating novel solutions. Borrowed understanding doesn't transfer because there's no underlying model to generalize from.

When I catch myself feeling knowledgeable about something I've only queried an AI about, I try to check myself: could I explain this without assistance? Could I apply it to a novel situation? Usually the honest answer is no.

## A Practical Experiment

Here's something I've been trying. Before asking an AI anything substantive, I spend a few minutes writing out my own thinking first:

1. What do I already know about this?
2. What are my current hypotheses?
3. What specific gaps am I trying to fill?

Only then do I consult the AI.

The practice serves multiple purposes. It forces me to generate before receiving. It makes the AI interaction more focused because I know what I'm looking for. And it creates a contrast between my thinking and the AI's that's often illuminating—either the AI confirms and extends my understanding, or it challenges it in ways I can now engage with critically.

The key is that the struggle happens *before* the assistance. The AI becomes a collaborator rather than a replacement.

## What's Worth Struggling With

I'm not arguing that all cognitive struggle is good. Struggling to remember a phone number in the age of contacts apps is pointless. Struggling to do arithmetic that a calculator handles instantly is mostly a waste.

The question is: which struggles are *constitutive* of the skill or understanding you're trying to develop?

If you're trying to become a better writer, struggling with sentence construction develops the skill in a way that accepting AI suggestions doesn't.

If you're trying to understand a domain deeply, wrestling with primary sources builds understanding that summaries can't.

If you're trying to develop judgment, making decisions yourself—even small ones—exercises the muscle.

The goal isn't to struggle for its own sake. It's to identify which struggles are actually building something, and protect those from optimization.

## The Bargain We're Making

Every technology involves bargains. Writing externalized memory, but may have weakened oral memory traditions. Calculators freed us from arithmetic drudgery, but most of us couldn't multiply large numbers by hand if we needed to.

AI is a bargain too. We gain speed, breadth, and capability. We risk depth, skill, and genuine understanding.

The bargain isn't necessarily bad. But it should be entered consciously. The question isn't whether to use AI—it's whether we're thinking clearly about what we're trading away.

The mind that struggles is the mind that grows. The challenge is to use AI in ways that preserve the struggles worth having.
